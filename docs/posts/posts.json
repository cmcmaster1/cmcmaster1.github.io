[
  {
    "path": "posts/2022-01-25-unbuilt-drug-safety-australias-digital-health-infrastructure/",
    "title": "Drug Safety & Australia's Digital Health Infrastructure",
    "description": "The problem of drug safety is a window into Australia's failure to invest in centralised digital health infrasctructure.",
    "author": [
      {
        "name": "Chris McMaster",
        "url": "https://chrismcmaster.com"
      }
    ],
    "date": "2022-01-30",
    "categories": [],
    "contents": "\n      \n        \n      \n    \n  ",
    "preview": "posts/2022-01-25-unbuilt-drug-safety-australias-digital-health-infrastructure/unbuilt-drug-safety-australias-digital-health-infrastructure_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-01-30T16:51:16+11:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-29-machine-learning-to-get-stuff-done/",
    "title": "Machine Learning To Get Stuff Done",
    "description": "How I read 38,000 PET-CT reports in a matter of minutes.",
    "author": [
      {
        "name": "Chris McMaster",
        "url": "https://chrismcmaster.com"
      }
    ],
    "date": "2022-01-29",
    "categories": [],
    "contents": "\n      \n        \n      \n    \n  ",
    "preview": "posts/2022-01-29-machine-learning-to-get-stuff-done/machine-learning-to-get-stuff-done_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-01-30T13:50:55+11:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-24-forecasting-the-future-in-rheumatoid-arthritis/",
    "title": "Forecasting the Future in Rheumatoid Arthritis",
    "description": "Predicting future RA disease activity.",
    "author": [
      {
        "name": "Chris McMaster",
        "url": "https://chrismcmaster.com"
      }
    ],
    "date": "2022-01-24",
    "categories": [],
    "contents": "\nIntroduction\nJust how good is machine learning at predicting the future? Leaving aside objectively hard problems involving dynamical systems (weather, stock markets etc.), what I am really interested in is the ability of deep learning to predict what will happen to my patients.\nImagine this situation. You’re seeing a patient with seropositive rheumatoid arthritis in clinic. They are currently on adalimumab 40mg every 2nd week and methotrexate 15mg weekly with folic acid. They have recently come off a course of prednisolone for a mild flare of their disease, but at the current visit they are in remission. Everything currently seems stable, you’re happy to see them in 3 months with the usual blood tests. But what if I were to tell you that, in 3 months time, there is a 90% probability that this patient will be experiencing a flare of their disease? Would that change what you do today? Would that change the timing of their next appointment?\nCurrently, we don’t yet have an evidence-based answer for how you should answer the above question, but that’s partly because we don’t have predictive tools that can tell us this probability with any great degree of accuracy.\nNorgeot and colleagues have attempted to create such a tool for rheumatoid arthritis. They have, in their own words, asked the question of whether “artificial intelligence models \\[can\\] prognosticate future patient outcomes for a complex disease, such as rheumatoid arthritis?” But they actually asked one further and very crucial question: does such a model generalise to other cohorts?\nI’m going to cover four topics in this post:\nWhat was their study design?\nWhat was their model? (RNN)\nHow to implement their model in R\nHow did they test it on a different cohort? (Transfer Learning)\nStudy Design\nPatients were selected from rheumatology clinics at two centres, UCSF hospital (UH) and Zuckerberg San Francisco General Hospital (SNH). The algorithm was initially trained on data from UCSF and subsequently tested on the ZH cohort using a concept called transfer learning (we will discuss this later).\nPatients\nPatients had to meet the following criteria:\nAt least two rheumatoid arthritis ICD codes\nThe prescription of a disease modifying anti-rheumatic drug (DMARD)\nAt least two recorded clinical disease activity index (CDAI) scores\nAt least one recorded C-reactive protein (CRP) level or erythrocyte sedimentation rate (ESR)\nIn total, 578 UH patients and 242 SNH patients were included.\nOutcome variable\nThe outcome they were trying to predict was a dichotomisation of the CDAI:\nControlled (CDAI≤10); or\nUncontrolled (CDAI>10)\nCovariates\nVariables were:\nClinical (previous CDAI score)\nTime-varying laboratory results (ESR and CRP)\nStatic laboratory results (rheumatoid factor, anticyclic citrullinated peptides)\nMedication (DMARDs, glucocorticoids)\nDemographic (age, sex, ethnicity)\nThe authors aren’t explicit in their manuscript, but I think this quote suggests that they used four visits as lead-in data and predicted the CDAI at the fifth visit:^[Under the section heading “Variables Used in the Model”:\n\nConsidering each variable at each of 4 different time windows resulted in a reasonably large total time-dependent variable space of 165 variables\n\nRecurrent Neural (RNNs)\nFor a long time (relatively speaking), recurrent neural networks have been the building blocks of a time-series deep learning model. There are several variants, but they all share the same basic principle.\n\nIn this diagram, h is one recurrent neuron, receiving inputs from the features x1, x2, … , xt, xt+1. Here, the subscript t is some sort of description of relative position - either time in the case of a time-series analysis, or position of a word in sentence when using these networks to model language.\nIn the paper we’re discussing, these features are things like C-reactive Protein (CRP), the swollen joint count and prednisolone dose. So we can imagine, for example, that x is the CRP and therefore these features the the CRP values at particular clinic visits. Each of these features has its own node within the recurrent neuron, known as the hidden state. The important thing here, and the feature that makes it a recurrent neural network, is that the hidden states themselves are linked via a “memory” gate (V). This connection between time points allows the model to learn features such as periodicity and trends.\nIn this paper, the authors have used a particular type of RNN called a Gated Recurrent Unit (GRU).\nSide note: there are two important historical problems with RNNs called the vanishing and exploding gradient problems. A note entirely explanation of this problem is this: the memory gate is a multiplier, with\nImplementation\nThis paper comes with code, available from their gitub repository. That is an exciting thing. They built their model in python, using keras, a popular API for building neural networks, particularly on top of Google’s deep learning software library, TensorFlow.\nI have translated their code into the {torch} package for R.\nStep 1: Load packages\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(torch)\nlibrary(luz)\n\n\n\nStep 2: Load the data\nI have made some simulation data of 100 patients with 10 visits each. For simplicity, I have not included DMARDs. The data here is mildly realistic only. Here is a small sample:\n\n\ntoy_data <- readRDS(\"toy_data.rds\") %>% \n  mutate(female = ifelse(sex == \"Female\", 1, 0))\n\n\n\n\n\n\npatient_id\n\n\nvisit_number\n\n\nage\n\n\nsex\n\n\nacpa\n\n\nrheum_factor\n\n\nCDAI\n\n\ndisease_control\n\n\nESR\n\n\nCRP\n\n\npnl\n\n\nfemale\n\n\n1\n\n\n1\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n11\n\n\n0\n\n\n18.5\n\n\n21.3\n\n\n30.0\n\n\n1\n\n\n1\n\n\n2\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n40\n\n\n0\n\n\n12.0\n\n\n0.0\n\n\n30.0\n\n\n1\n\n\n1\n\n\n3\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n34\n\n\n0\n\n\n12.0\n\n\n0.0\n\n\n30.0\n\n\n1\n\n\n1\n\n\n4\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n5\n\n\n1\n\n\n12.0\n\n\n0.0\n\n\n30.0\n\n\n1\n\n\n1\n\n\n5\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n38\n\n\n0\n\n\n9.2\n\n\n0.0\n\n\n27.5\n\n\n1\n\n\n1\n\n\n6\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n17\n\n\n0\n\n\n7.3\n\n\n0.0\n\n\n25.0\n\n\n1\n\n\n1\n\n\n7\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n4\n\n\n1\n\n\n8.2\n\n\n0.0\n\n\n25.0\n\n\n1\n\n\n1\n\n\n8\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n28\n\n\n0\n\n\n8.2\n\n\n0.0\n\n\n25.0\n\n\n1\n\n\n1\n\n\n9\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n19\n\n\n0\n\n\n8.2\n\n\n0.0\n\n\n25.0\n\n\n1\n\n\n1\n\n\n10\n\n\n58\n\n\nFemale\n\n\n39\n\n\n5\n\n\n5\n\n\n1\n\n\n8.2\n\n\n0.0\n\n\n25.0\n\n\n1\n\n\n2\n\n\n1\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n4\n\n\n1\n\n\n25.3\n\n\n29.5\n\n\n25.0\n\n\n0\n\n\n2\n\n\n2\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n0\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n25.0\n\n\n0\n\n\n2\n\n\n3\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n0\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n25.0\n\n\n0\n\n\n2\n\n\n4\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n0\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n25.0\n\n\n0\n\n\n2\n\n\n5\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n0\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n25.0\n\n\n0\n\n\n2\n\n\n6\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n0\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n30.0\n\n\n0\n\n\n2\n\n\n7\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n0\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n30.0\n\n\n0\n\n\n2\n\n\n8\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n0\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n30.0\n\n\n0\n\n\n2\n\n\n9\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n0\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n30.0\n\n\n0\n\n\n2\n\n\n10\n\n\n56\n\n\nMale\n\n\n9\n\n\n172\n\n\n0\n\n\n1\n\n\n0.0\n\n\n0.0\n\n\n30.0\n\n\n0\n\n\n3\n\n\n1\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n18\n\n\n0\n\n\n60.4\n\n\n64.2\n\n\n15.0\n\n\n1\n\n\n3\n\n\n2\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n17\n\n\n0\n\n\n80.1\n\n\n44.7\n\n\n7.5\n\n\n1\n\n\n3\n\n\n3\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n11\n\n\n0\n\n\n89.4\n\n\n58.2\n\n\n7.5\n\n\n1\n\n\n3\n\n\n4\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n19\n\n\n0\n\n\n89.4\n\n\n58.2\n\n\n7.5\n\n\n1\n\n\n3\n\n\n5\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n19\n\n\n0\n\n\n89.4\n\n\n58.2\n\n\n7.5\n\n\n1\n\n\n3\n\n\n6\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n6\n\n\n1\n\n\n89.4\n\n\n58.2\n\n\n7.5\n\n\n1\n\n\n3\n\n\n7\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n22\n\n\n0\n\n\n89.4\n\n\n58.2\n\n\n7.5\n\n\n1\n\n\n3\n\n\n8\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n23\n\n\n0\n\n\n89.4\n\n\n58.2\n\n\n7.5\n\n\n1\n\n\n3\n\n\n9\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n8\n\n\n1\n\n\n89.4\n\n\n58.2\n\n\n7.5\n\n\n1\n\n\n3\n\n\n10\n\n\n54\n\n\nFemale\n\n\n110\n\n\n206\n\n\n20\n\n\n0\n\n\n83.6\n\n\n60.9\n\n\n12.5\n\n\n1\n\n\n4\n\n\n1\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n7\n\n\n1\n\n\n36.3\n\n\n38.0\n\n\n7.5\n\n\n1\n\n\n4\n\n\n2\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n7\n\n\n1\n\n\n20.2\n\n\n39.2\n\n\n12.5\n\n\n1\n\n\n4\n\n\n3\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n5\n\n\n1\n\n\n7.2\n\n\n25.6\n\n\n7.5\n\n\n1\n\n\n4\n\n\n4\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n0\n\n\n1\n\n\n15.3\n\n\n35.0\n\n\n7.5\n\n\n1\n\n\n4\n\n\n5\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n2\n\n\n1\n\n\n15.3\n\n\n35.0\n\n\n7.5\n\n\n1\n\n\n4\n\n\n6\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n4\n\n\n1\n\n\n15.3\n\n\n35.0\n\n\n7.5\n\n\n1\n\n\n4\n\n\n7\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n6\n\n\n1\n\n\n15.3\n\n\n35.0\n\n\n7.5\n\n\n1\n\n\n4\n\n\n8\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n1\n\n\n1\n\n\n15.3\n\n\n35.0\n\n\n7.5\n\n\n1\n\n\n4\n\n\n9\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n10\n\n\n1\n\n\n17.8\n\n\n37.1\n\n\n10.0\n\n\n1\n\n\n4\n\n\n10\n\n\n48\n\n\nFemale\n\n\n36\n\n\n267\n\n\n1\n\n\n1\n\n\n18.6\n\n\n36.6\n\n\n15.0\n\n\n1\n\n\n5\n\n\n1\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n2\n\n\n1\n\n\n47.9\n\n\n52.8\n\n\n15.0\n\n\n1\n\n\n5\n\n\n2\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n5\n\n\n1\n\n\n33.8\n\n\n77.2\n\n\n15.0\n\n\n1\n\n\n5\n\n\n3\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n8\n\n\n1\n\n\n33.8\n\n\n77.2\n\n\n15.0\n\n\n1\n\n\n5\n\n\n4\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n6\n\n\n1\n\n\n28.5\n\n\n73.9\n\n\n12.5\n\n\n1\n\n\n5\n\n\n5\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n1\n\n\n1\n\n\n33.5\n\n\n75.4\n\n\n12.5\n\n\n1\n\n\n5\n\n\n6\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n6\n\n\n1\n\n\n28.2\n\n\n72.1\n\n\n10.0\n\n\n1\n\n\n5\n\n\n7\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n8\n\n\n1\n\n\n33.3\n\n\n73.6\n\n\n10.0\n\n\n1\n\n\n5\n\n\n8\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n1\n\n\n1\n\n\n33.3\n\n\n73.6\n\n\n10.0\n\n\n1\n\n\n5\n\n\n9\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n3\n\n\n1\n\n\n33.3\n\n\n73.6\n\n\n10.0\n\n\n1\n\n\n5\n\n\n10\n\n\n56\n\n\nFemale\n\n\n60\n\n\n143\n\n\n9\n\n\n1\n\n\n28.0\n\n\n70.3\n\n\n7.5\n\n\n1\n\n\n6\n\n\n1\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n10\n\n\n1\n\n\n30.9\n\n\n35.4\n\n\n7.5\n\n\n0\n\n\n6\n\n\n2\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n2\n\n\n1\n\n\n2.9\n\n\n26.3\n\n\n7.5\n\n\n0\n\n\n6\n\n\n3\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n13\n\n\n0\n\n\n1.3\n\n\n29.7\n\n\n12.5\n\n\n0\n\n\n6\n\n\n4\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n16\n\n\n0\n\n\n0.0\n\n\n25.6\n\n\n12.5\n\n\n0\n\n\n6\n\n\n5\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n10\n\n\n1\n\n\n0.0\n\n\n25.6\n\n\n12.5\n\n\n0\n\n\n6\n\n\n6\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n20\n\n\n0\n\n\n0.0\n\n\n25.6\n\n\n12.5\n\n\n0\n\n\n6\n\n\n7\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n15\n\n\n0\n\n\n0.0\n\n\n25.6\n\n\n12.5\n\n\n0\n\n\n6\n\n\n8\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n21\n\n\n0\n\n\n0.0\n\n\n22.1\n\n\n7.5\n\n\n0\n\n\n6\n\n\n9\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n8\n\n\n1\n\n\n1.3\n\n\n29.7\n\n\n12.5\n\n\n0\n\n\n6\n\n\n10\n\n\n57\n\n\nMale\n\n\n29\n\n\n205\n\n\n13\n\n\n0\n\n\n0.0\n\n\n25.6\n\n\n12.5\n\n\n0\n\n\n7\n\n\n1\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n4\n\n\n1\n\n\n35.8\n\n\n40.6\n\n\n15.0\n\n\n0\n\n\n7\n\n\n2\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n35\n\n\n0\n\n\n12.8\n\n\n30.7\n\n\n15.0\n\n\n0\n\n\n7\n\n\n3\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n4\n\n\n1\n\n\n12.8\n\n\n30.7\n\n\n15.0\n\n\n0\n\n\n7\n\n\n4\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n12\n\n\n0\n\n\n12.8\n\n\n30.7\n\n\n15.0\n\n\n0\n\n\n7\n\n\n5\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n24\n\n\n0\n\n\n12.8\n\n\n30.7\n\n\n15.0\n\n\n0\n\n\n7\n\n\n6\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n27\n\n\n0\n\n\n13.3\n\n\n14.9\n\n\n10.0\n\n\n0\n\n\n7\n\n\n7\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n30\n\n\n0\n\n\n19.0\n\n\n30.5\n\n\n12.5\n\n\n0\n\n\n7\n\n\n8\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n24\n\n\n0\n\n\n16.0\n\n\n26.6\n\n\n12.5\n\n\n0\n\n\n7\n\n\n9\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n3\n\n\n1\n\n\n15.5\n\n\n42.4\n\n\n17.5\n\n\n0\n\n\n7\n\n\n10\n\n\n71\n\n\nMale\n\n\n48\n\n\n189\n\n\n20\n\n\n0\n\n\n8.9\n\n\n50.4\n\n\n22.5\n\n\n0\n\n\n8\n\n\n1\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n7\n\n\n1\n\n\n21.2\n\n\n22.6\n\n\n7.5\n\n\n1\n\n\n8\n\n\n2\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n1\n\n\n1\n\n\n4.9\n\n\n15.7\n\n\n7.5\n\n\n1\n\n\n8\n\n\n3\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n0\n\n\n1\n\n\n4.9\n\n\n15.7\n\n\n7.5\n\n\n1\n\n\n8\n\n\n4\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n0\n\n\n1\n\n\n4.9\n\n\n15.7\n\n\n7.5\n\n\n1\n\n\n8\n\n\n5\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n0\n\n\n1\n\n\n3.0\n\n\n15.6\n\n\n5.0\n\n\n1\n\n\n8\n\n\n6\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n0\n\n\n1\n\n\n6.1\n\n\n19.3\n\n\n5.0\n\n\n1\n\n\n8\n\n\n7\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n1\n\n\n1\n\n\n9.8\n\n\n19.4\n\n\n10.0\n\n\n1\n\n\n8\n\n\n8\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n0\n\n\n1\n\n\n0.0\n\n\n11.9\n\n\n5.0\n\n\n1\n\n\n8\n\n\n9\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n0\n\n\n1\n\n\n4.3\n\n\n19.2\n\n\n2.5\n\n\n1\n\n\n8\n\n\n10\n\n\n66\n\n\nFemale\n\n\n4\n\n\n129\n\n\n1\n\n\n1\n\n\n7.3\n\n\n22.8\n\n\n2.5\n\n\n1\n\n\n9\n\n\n1\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n7\n\n\n1\n\n\n36.3\n\n\n38.7\n\n\n15.0\n\n\n1\n\n\n9\n\n\n2\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n8\n\n\n1\n\n\n19.6\n\n\n0.0\n\n\n10.0\n\n\n1\n\n\n9\n\n\n3\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n6\n\n\n1\n\n\n25.3\n\n\n4.0\n\n\n10.0\n\n\n1\n\n\n9\n\n\n4\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n2\n\n\n1\n\n\n25.3\n\n\n4.0\n\n\n10.0\n\n\n1\n\n\n9\n\n\n5\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n2\n\n\n1\n\n\n13.4\n\n\n0.0\n\n\n5.0\n\n\n1\n\n\n9\n\n\n6\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n0\n\n\n1\n\n\n13.2\n\n\n3.0\n\n\n2.5\n\n\n1\n\n\n9\n\n\n7\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n6\n\n\n1\n\n\n10.1\n\n\n3.8\n\n\n0.0\n\n\n1\n\n\n9\n\n\n8\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n7\n\n\n1\n\n\n13.0\n\n\n7.2\n\n\n0.0\n\n\n1\n\n\n9\n\n\n9\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n2\n\n\n1\n\n\n13.0\n\n\n7.2\n\n\n0.0\n\n\n1\n\n\n9\n\n\n10\n\n\n58\n\n\nFemale\n\n\n93\n\n\n102\n\n\n1\n\n\n1\n\n\n19.0\n\n\n9.8\n\n\n2.5\n\n\n1\n\n\n10\n\n\n1\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n1\n\n\n1\n\n\n37.4\n\n\n43.6\n\n\n15.0\n\n\n0\n\n\n10\n\n\n2\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n3\n\n\n1\n\n\n14.4\n\n\n33.2\n\n\n20.0\n\n\n0\n\n\n10\n\n\n3\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n0\n\n\n1\n\n\n6.0\n\n\n26.5\n\n\n20.0\n\n\n0\n\n\n10\n\n\n4\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n1\n\n\n1\n\n\n2.6\n\n\n26.3\n\n\n17.5\n\n\n0\n\n\n10\n\n\n5\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n10\n\n\n1\n\n\n13.6\n\n\n30.0\n\n\n22.5\n\n\n0\n\n\n10\n\n\n6\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n1\n\n\n1\n\n\n12.0\n\n\n23.5\n\n\n27.5\n\n\n0\n\n\n10\n\n\n7\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n3\n\n\n1\n\n\n10.4\n\n\n17.0\n\n\n32.5\n\n\n0\n\n\n10\n\n\n8\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n0\n\n\n1\n\n\n1.9\n\n\n10.3\n\n\n32.5\n\n\n0\n\n\n10\n\n\n9\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n7\n\n\n1\n\n\n0.0\n\n\n10.0\n\n\n27.5\n\n\n0\n\n\n10\n\n\n10\n\n\n61\n\n\nMale\n\n\n40\n\n\n150\n\n\n5\n\n\n1\n\n\n3.6\n\n\n16.7\n\n\n27.5\n\n\n0\n\n\n\nWe will split into train and test sets.\nTo use this in a multivariable RNN, the data needs to be in two variables:\nA 3D array of input features, of shape \\[samples, time steps, features\\]\nA 1D array of outcomes\nThese will then be placed into dataloaders, so the data can be fed into the model in batches (here we’re feeding in 100 samples at a time).\n\n\nids <- toy_data %>% \n  select(patient_id) %>% \n  distinct() %>% \n  rowwise() %>% \n  mutate(rand = runif(1)) %>% \n  mutate(group = case_when(rand >= 0.85 ~ \"Valid\", \n                           rand >= 0.7 ~ \"Test\", \n                           TRUE ~ \"Train\"))\ntrain_ids <- ids %>% filter(group == \"Train\") %>% pull(patient_id)\nvalid_ids <- ids %>% filter(group == \"Valid\") %>% pull(patient_id)\ntest_ids <- ids %>% filter(group == \"Test\") %>% pull(patient_id)\n\ntoy_dataset <- dataset(\n  name = \"toy_dataset\",\n  \n  initialize = function(\n    x,\n    ids,\n    n_timesteps = 10, \n    pred_window = 4) {\n    \n    self$x <- x %>% \n      filter(patient_id %in% ids)\n    self$pred_window <- pred_window\n    starting_n <- self$x %>% \n      mutate(row = row_number()) %>% \n      filter(visit_number < 7) %>% \n      pull(row)\n    self$starts <- sample(starting_n)\n    self$len <- length(self$starts)\n    \n  },\n  \n  .getitem = function(i) {\n    \n    start <- self$starts[i]\n    end <- start + self$pred_window - 1\n    \n    x <- self$x %>% \n      select(age, acpa, rheum_factor, CDAI, female,\n             ESR, CRP, pnl) %>% \n      slice(start:end) %>% \n      as.matrix()\n    \n    y <- self$x %>% \n      slice(end+1) %>% \n      pull(disease_control)\n    \n    list(x, y)\n    \n  },\n  \n  .length = function() {\n    self$len\n  }\n)\n\ntrain_dset <- toy_dataset(toy_data, ids = train_ids)\ntest_dset <- toy_dataset(toy_data, ids = test_ids)\nvalid_dset <- toy_dataset(toy_data, ids = valid_ids)\ntrain_dl <- train_dset %>% dataloader(batch_size = 100, shuffle = TRUE)\ntest_dl <- test_dset %>% dataloader(batch_size = 100, shuffle = FALSE)\nvalid_dl <- valid_dset %>% dataloader(batch_size = 100, shuffle = FALSE)\n\n\n\nStep 3: Create the model function\nThis function, creatively named model, takes five parameters: the type of recurrent layer (“gru” or “lstm”), the input size of our data (number of variables), desired dimensions of the recurrent layer (hidden_size), the number of recurrent layers and the dropout fraction.\n\n\nmodel <- nn_module(\n  \n  initialize = function(\n    type, \n    input_size, \n    hidden_size, \n    num_layers = 1, \n    dropout = 0,\n    load_from = NA,\n    freeze_params = NA) {\n    \n    if (!is.na(load_from)){\n      saved_model <- torch::torch_load(load_from)\n      self$load_state_dict(saved_model$state_dict())\n    }\n    \n    if (!is.na(freeze_params)){\n      self$freeze_weights(freeze_params)\n    }\n    \n    self$type <- type\n    self$num_layers <- num_layers\n    \n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    }\n    \n    half_size <- round(hidden_size / 2)\n    \n    self$mod_list <- nn_module_list(\n      list(\n        nn_dropout(dropout),\n        nn_linear(hidden_size, half_size),\n        nn_dropout(dropout),\n        nn_linear(half_size, 1)\n      )\n    )\n    \n  },\n  \n  forward = function(batch) {\n    \n    x <- self$rnn(batch)[[1]] # (batch_size, n_timesteps, hidden_size)\n    x <- x[ , dim(x)[2], ] # (batch_size, hidden_size)\n    \n    for (i in 1:length(self$mod_list)){\n      x <- self$mod_list[[i]](x)\n    }\n    x[,1]\n    \n  },\n  \n  freeze_weights = function(regex){\n    names <- names(self$parameters)\n    names <- names[str_detect(names, regex)]\n    for (name in names){\n      self$parameters[[name]]$requires_grad_(FALSE)\n    }\n  }\n  \n)\n\n\n\nStep 4: Build the model\n\n\nmodel_setup <- model %>%\n  setup(\n    loss = nn_bce_with_logits_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_binary_accuracy_with_logits(),\n      luz_metric_binary_auroc(from_logits = TRUE)\n    )\n  ) %>%\n  set_hparams(type = \"gru\", \n              input_size = 8, \n              hidden_size = 32, \n              num_layers = 2, \n              dropout = 0.1) %>%\n  set_opt_hparams(lr = 0.01)\n\n\n\nStep 5: Train the model\n\n\nfitted <- fit(model_setup,\n              train_dl, \n              epochs = 25, \n              valid_data = valid_dl, \n              verbose = FALSE)\n\n\n\n\n\n\nWe can save this model for future use.\n\n\nluz_save_model_weights(fitted, \"initial_fit.RDS\")\n\n\n\nStep 6: Evaluate the model\nWe can evaluate our model using the area under the receiver operating characteristic.\n\n\nmetrics <- evaluate(fitted, test_dset)$records$metrics$valid[[1]]\ntruth <- factor(test_dset$.getitem(1:1008)[[2]], levels = c(1, 0))\npredictions <- as_array(nnf_sigmoid(predict(fitted, test_dset)))\nroc_data <- tibble(truth, predictions)\nroc_curve <- roc_curve(roc_data, truth = truth, estimate = predictions)\nautoplot(roc_curve)\n\n\n\n\nThe AUC is 0.81 - this seems fairly good. But how would our model perform on new data?\nLet’s load a new dataset and again split into training and test sets.\n\n\nnew_toy_data <- readRDS(\"new_toy_data.rds\") %>% \n  mutate(female = ifelse(sex == \"Female\", 1, 0))\n\nnew_dset <- toy_dataset(new_toy_data, ids = unique(new_toy_data$patient_id))\nnew_dl <- as_dataloader(new_dset, batch_size = 100)\n\n\n\nWe have loaded our new dataset, which you is a lot smaller. We will test our model and see how it performs.\n\n\nmetrics <- evaluate(fitted, new_dset)$records$metrics$valid[[1]]\ntruth <- factor(new_dset$.getitem(1:new_dset$.length())[[2]], levels = c(1, 0))\npredictions <- as_array(nnf_sigmoid(predict(fitted, new_dset)))\nroc_data <- tibble(truth, predictions)\nroc_curve <- roc_curve(roc_data, truth = truth, estimate = predictions)\nautoplot(roc_curve)\n\n\n\n\nOh no, the AUC is only 0.72 - that’s not great! How can we do better? Well, we can do what the authors of this paper did and perform transfer learning.\nTransfer Learning\nTransfer learning is the process of taking a model trained on one task and reapplying it to perform a new task, often with some re-training on the new task, although sometimes without. Generally, we perform transfer learning with models originally trained on large datasets and transfer those models to tasks using much smaller datasets. In doing so, we don’t want to lose all of the hard-won information learnt from the large dataset, so re-training is generally performed on a subset of the model layers (the latter layers). We will perform transfer learning by freezing all layers, except the dense layers.\nFirst, we will do a data split like earlier:\n\n\nids <- new_toy_data %>% \n  select(patient_id) %>% \n  distinct() %>% \n  rowwise() %>% \n  mutate(rand = runif(1)) %>% \n  mutate(group = case_when(rand >= 0.8 ~ \"Valid\", \n                           rand >= 0.6 ~ \"Test\", \n                           TRUE ~ \"Train\"))\ntrain_ids <- ids %>% filter(group == \"Train\") %>% pull(patient_id)\nvalid_ids <- ids %>% filter(group == \"Valid\") %>% pull(patient_id)\ntest_ids <- ids %>% filter(group == \"Test\") %>% pull(patient_id)\n\ntrain_dset <- toy_dataset(new_toy_data, ids = train_ids)\ntest_dset <- toy_dataset(new_toy_data, ids = test_ids)\nvalid_dset <- toy_dataset(new_toy_data, ids = valid_ids)\ntrain_dl <- train_dset %>% dataloader(batch_size = 100, shuffle = TRUE)\ntest_dl <- test_dset %>% dataloader(batch_size = 100, shuffle = FALSE)\nvalid_dl <- valid_dset %>% dataloader(batch_size = 100, shuffle = FALSE)\n\n\n\nNow we will create a function to freeze the RNN:\n\n\nmodel_new <- model %>%\n  setup(\n    loss = nn_bce_with_logits_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_binary_accuracy_with_logits(),\n      luz_metric_binary_auroc(from_logits = TRUE)\n    )\n  ) %>%\n  set_hparams(type = \"gru\", \n              input_size = 8, \n              hidden_size = 32, \n              num_layers = 2, \n              dropout = 0.1,\n              load_from = \"initial_fit.RDS\",\n              freeze_params = \"rnn\") %>%\n  set_opt_hparams(lr = 0.01)\n\n\n\nNow let’s train the final deep layers:\n\n\nfitted_new <- fit(model_new,\n                  train_dl, \n                  epochs = 25, \n                  valid_data = valid_dl, \n                  verbose = FALSE)\n\n\n\n\n\n\nLooking now at the performance:\n\n\nmetrics <- evaluate(fitted, test_dset)$records$metrics$valid[[1]]\ntruth <- factor(test_dset$.getitem(1:1008)[[2]], levels = c(1, 0))\npredictions <- as_array(nnf_sigmoid(predict(fitted, test_dset)))\nroc_data <- tibble(truth, predictions)\nroc_curve <- roc_curve(roc_data, truth = truth, estimate = predictions)\nautoplot(roc_curve)\n\n\n\n\nThe AUC has now risen to 0.78 - a little bit better.\nWrapping Up\nPredicting the future in rheumatoid arthritis may every well expand our horizons for optimal disease management. Pre-emptive management ought to be one of the cornerstones of treat-to-target strategies, and yet pre-emptive management is unattainable without accurate prediction models. Whilst machine learning offers an opportunity to build such models, the model we have explored here has some shortcomings. In particular, it remains a black box in terms of how it comes to its conclusions (explain-ability) and how certain it is of these conclusions (confidence).\n\n\n\n",
    "preview": "posts/2022-01-24-forecasting-the-future-in-rheumatoid-arthritis/forecasting-the-future-in-rheumatoid-arthritis_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-01-25T11:00:00+11:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
