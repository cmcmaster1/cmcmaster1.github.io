---
title: "A Primer on NLP for Clinicians: Part 1 - Tokenization"
description: |
  In this first part we talk about tokenization, the tool we use to make text readable by a machine.
author:
  - name: Chris McMaster
    url: {}
date: 2022-02-12
output:
  distill::distill_article:
    self_contained: false
    highlight: breezedark
    includes:
        in_header: banner.html
draft: false
preview: BPE.drawio.svg
css: banner.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, highlight = TRUE, class.source="bg-success")
```

```{js}
const expandElements = shouldExpand => {
        let detailsElements = document.querySelectorAll("details");
        
        detailsElements = [...detailsElements];

        if (shouldExpand) {
            detailsElements.map(item => item.setAttribute("open", shouldExpand));
        } else {
            detailsElements.map(item => item.removeAttribute("open"));
        }
    };

```

```{r, echo=FALSE, results='asis'}
html <- '
<button onClick="expandElements(true)">Expand</button>
<button onClick="expandElements(false)">Collapse</button>
'
cat(html)
```

# What's this and why should I care?

If you're reading this, I presume you have at least a cursory interest in natural language processing (NLP). Either that or I have made you read this. Regardless, this primer on NLP for clinicians will show you how modern NLP works and how it can be applied in clinical medicine. The reasons *why* are almost too many to mention. Aside from the interesting knowledge we might gain, NLP is an extremely practical tool. We spend our lives reading text --- journals, articles, progress notes, you name it, we're constantly reading to absorb new information. So much of this can be simplified, streamlined, automated and improved with NLP. From conducting audits to summarising the latest literature, NLP can make our lives easier. Hopefully this sparks some ideas and becomes the start of that path for you. We will start with tokenization.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(DiagrammeR)


grViz(diagram = "digraph flowchart {
  node [fontname = arial, shape = oval, color = LightSalmon, style = filled]
  tab1 [label = '@@1']
  
  node [fontname = arial, shape = oval, color = Grey]
  tab2 [label = '@@2']
  tab3 [label = '@@3']
  
  tab1 -> tab2 -> tab3;
}
  
  [1]: 'Tokenization'
  [2]: 'Embeddings'    
  [3]: 'Transformers'    
  ",
  height = 200
)
```

```{python include=FALSE}
import pprint
pp = pprint.PrettyPrinter(indent=0, width=80, compact=True)
def print(string):
  pp.pprint(string)
notes = [
"HOPC: 2 weeks of progressive exertional dysponea, now SOB at rest w/ maximum ET ~10m. Periphral oedema w/ pitting above knees bilaterally.",
"3 days of progressive exertional dysponea, now SOB at rest w/ max exercise tolerance 20m. Peripheral oedema w/ pitting above both knees.",
"The patient is a 38-year-old female who reports fatigue, SOB and flu-like symptoms for the past two weeks.",
"She has a temperature of 37°C and reports significant fatigue and body aches.",
"She has also developed a rash on her chest and her neck.",
"She is generally feeling quite unwell and would like to know what might be wrong with her.",
"She denies any other significant medical history and takes no regular medications.",
"On examination, she is generally swollen with oedema noted in both ankles and feels especially tender on examination of her chest and neck.",
"She also has a diffuse red rash on her torso and a mild fever of 37°C.",
"Laboratory investigations reveal a white blood cell count of 15,000/mm3 and a C-reactive protein level of 112.",
"She is diagnosed with a viral infection and is started on oral ibuprofen and bed rest.",
"The patient is advised to follow up with her primary care physician in one week's time if her symptoms have not improved."
]
```

# Tokenization

Let's pretend we have a large number of clinical notes (we will refer to this as a ***corpus***) and we want to represent those in such a way that they can be used by a machine learning model. We can think about this by examining the first snippet from our corpus (there are 10 other brief notes that I won't show):

```{python echo=FALSE}
print(notes[0])
```

One natural way to turn this text into data might be to split it every time we encounter a space, essentially splitting into words. For example:

```{python echo=FALSE}
print(notes[0].split())
```

Each word is a data point, we could even convert this into a table and feed it in that way. Some cleverer ideas involve counting the frequency of each word and using that data (see [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), although import information is lost in this process, namely the position of each word).

Our simple tokenizer has problems, particularly as the number of documents begins to explode and therefore the number of unique words becomes unreasonably large. Additionally, each misspelling will be its own unique token, which might be okay for very common typos, but we would ideally like some way of coping with small variations in spelling that might occur less frequently.

Subword tokenization refers to a group of tokenization techniques that allow us to optimally tokenize a corpus of text with a limited vocabulary (where we get to choose how large that is). We're going to explain this using the simplest subword tokenizer, **byte pair encoding (BPE)**.

## Byte Pair Encoding

We will get into the details of Byte Pair Encoding (BPE) later. For now, if you're following along with the code we will import our libraries and instantiate a BPE.

```{python echo=TRUE, code_folding=TRUE}
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer

tokenizer = Tokenizer(BPE())
```

Before we can train a BPE algorithm, we need to clean up our text. We will do this in two steps:

1.  Normalization

2.  Pre-processing

### Normalization

Normalization is the process of removing and replacing characters in text to reduce it to a standard set of characters. This often involves several steps and might include the following:

-   Removing unwanted characters (e.g. emoji or cyrillic characters)

-   [Normalizing unicode characters](https://unicode.org/reports/tr15/) by decomposing them. A common example is accented letters, which might be replaced with their unaccented form plus the accent (e.g. replace Á with A´)

-   Replacing all uppercase letters with their lowercase form (e.g. replace B with b)

-   Removing those accents created through unicode normalisation

Here, we create a normalization schema by stringing together NFD (a unicode normalization standard - we don't need to know the details), the lowercase function and a function to remove those accents.

```{python echo=TRUE}
tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])
```

This is what our string looks like after normalization:

```{python echo=FALSE}
normalized = tokenizer.normalizer.normalize_str(notes[0])
print(normalized)
```

### Pre-processing

We can now pre-process our text --- that is, we can split it into parts. These parts are the maximum units we can tokenize, with the rules that each part is composed of 1 or more tokens, but no token can span between parts. If you you think about it, what we're really describing here are words --- we're going to split our text into words. We will do this by splitting our text every time we encounter whitespace, although you can also include rules about punctuation.

```{python echo=TRUE}
tokenizer.pre_tokenizer = Whitespace()
```

So here our text becomes:

```{python echo=FALSE}
pre_tokenized = tokenizer.pre_tokenizer.pre_tokenize_str(normalized)
elements, positions = zip(*pre_tokenized)
print(elements)
```

### BPE

We're now ready to train a tokenizer, so lets try and understand the algorithm.

After pre-processing, we count the number of instances for each word --- this is just going to help us with calculations down the line. We then begin with a candidate vocabulary consisting of every character that appears at least once in our corpus. We then look at token pairs, that is two consecutive tokens occurring within a word. We count the number of times each token pair appears. The token pair that appears most frequently gets added to our vocabulary (in this example *at* and *on* share the highest frequency and in reality adding each to our vocabulary occurs in 2 separate steps, condensed to 1 step here for brevity). This process then repeats, with our new tokens also counted at the next.

![](BPE.drawio.svg)

This process continues until we either run out of new tokens to create (i.e. each word is its own token), or we reach our maximum vocabulary size.

We can now see what this looks like with our corpus. Our corpus is very small, so we will use a vocabulary of size 150, although for an appropriately large corpus we usually use a vocabulary in the 10s of thousands.

```{python echo=TRUE}
trainer = BpeTrainer(vocab_size=150)
tokenizer.train_from_iterator(notes, trainer)
```

We can look at the first 10 tokens in the vocabulary:

```{python echo=FALSE}
vocab = tokenizer.get_vocab() # Get the full vocabulary as a dictionary
print(list(vocab.keys())[:10]) # Print the first 10 tokens of the vocabulary
```

As you can see, these aren't words. They are parts of words, carefully chosen by our algorithm to tokenize the most frequently occurring parts within our vocabulary limit.

Finally, we can see what our text looks like after tokenization:

```{python echo=FALSE}
encoded = tokenizer.encode(notes[0])
print(encoded.tokens)
```

Actually, this isn't what the machine learning model sees. Each token in our vocabulary is represented by a number, so this is what it will see:

```{python echo=FALSE}
encoded = tokenizer.encode(notes[0])
print(encoded.ids)
```

# Next Steps

That's tokenization at a basic level. Next, we need to find a way to make these tokens meaningful with embeddings.
